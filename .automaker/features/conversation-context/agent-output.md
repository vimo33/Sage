
ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "content": "Explore codebase to understand existing chat/context structure",
      "status": "in_progress",
      "activeForm": "Exploring codebase for chat/context structure"
    },
    {
      "content": "Identify where context window is defined and how messages are managed",
      "status": "pending",
      "activeForm": "Identifying context window configuration"
    },
    {
      "content": "Implement expansion from 10 to 20 messages with smart summarization",
      "status": "pending",
      "activeForm": "Implementing context window expansion"
    },
    {
      "content": "Verify implementation with Playwright test",
      "status": "pending",
      "activeForm": "Verifying with Playwright test"
    }
  ]
}
Let me explore the codebase to understand the existing chat and context structure.

ðŸ”§ Tool: Task
Input: {
  "description": "Explore chat context structure",
  "prompt": "Explore this codebase to understand:\n1. How the chat system is structured\n2. Where conversation context/history is managed\n3. Where any context window limits (like number of messages) are defined\n4. How messages are stored and retrieved\n\nFocus on the lib/chat/ directory and any related files. Look for constants, configurations, or logic that controls how many messages are kept in context.\n\nReport back with:\n- Key files involved in chat/context management\n- Any existing context window limits or message history configurations\n- The overall architecture of the chat system",
  "subagent_type": "Explore"
}

ðŸ”§ Tool: Glob
Input: {
  "pattern": "lib/chat/**/*"
}

ðŸ”§ Tool: Glob
Input: {
  "pattern": "lib/**/*.ts"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/index.ts"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/service.ts"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/sage.ts"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/storage/store.ts"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/llm/inference.ts"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/retrieval/search.ts"
}

ðŸ”§ Tool: Glob
Input: {
  "pattern": "app/**/*.tsx"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "grep -r \"generateAssistantResult\\|generateSageReply\\|chatHistory\\|addChatMessage\" /Users/vimo/Projects/OpenCode/Sage_AI --include=\"*.tsx\" --include=\"*.ts\" 2>/dev/null | head -30",
  "description": "Search for chat system usage in app files"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/app/ask.tsx"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/package.json"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "grep -r \"context\\|window\\|limit\\|CONTEXT\\|WINDOW\" /Users/vimo/Projects/OpenCode/Sage_AI/lib --include=\"*.ts\" 2>/dev/null | grep -i \"const\\|let\\|export\" | head -20",
  "description": "Search for context/window limit definitions"
}

ðŸ”§ Tool: Bash
Input: {
  "command": "grep -n \"nCtx\\|n_ctx\\|2048\\|slice.*-10\\|previousMessages\" /Users/vimo/Projects/OpenCode/Sage_AI/lib -r --include=\"*.ts\" 2>/dev/null",
  "description": "Find context window and message slicing definitions"
}
Excellent! I now have a clear understanding of the codebase. Let me mark the first task as complete and update my progress.

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "content": "Explore codebase to understand existing chat/context structure",
      "status": "completed",
      "activeForm": "Exploring codebase for chat/context structure"
    },
    {
      "content": "Identify where context window is defined and how messages are managed",
      "status": "completed",
      "activeForm": "Identifying context window configuration"
    },
    {
      "content": "Implement expansion from 10 to 20 messages with smart summarization",
      "status": "in_progress",
      "activeForm": "Implementing context window expansion"
    },
    {
      "content": "Verify implementation with Playwright test",
      "status": "pending",
      "activeForm": "Verifying with Playwright test"
    }
  ]
}
Let me read the key files to understand the exact implementation details:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/service.ts"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/storage/store.ts"
}
Now I have a clear picture of the implementation. The key findings:

1. **Context window limit**: Line 112 in `service.ts` has `.slice(-10)` which limits to 10 messages
2. **Token context**: 2048 tokens set in the model initialization (lines 217, 313 in `store.ts`)

For smart summarization, I need to:
1. Expand the context window from 10 to 20 messages
2. Add a summarization mechanism for older messages when context gets too large

Let me implement the feature:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/service.ts",
  "old_string": "import type { RNLlamaOAICompatibleMessage } from 'llama.rn';\n\nimport { generateChat, isModelReady } from '../llm/inference';\nimport { detectIntentThemes, searchWisdom, type SearchOptions, type ToneTag, type WisdomChunk } from '../retrieval/search';\nimport type { TonePreference, UserPreferences } from '../storage/store';\n\nexport type CitedVerse = { content: string; sourceRef: string };\n\nexport type AssistantResult = {\n  content: string;\n  citedVerses: CitedVerse[];\n};",
  "new_string": "import type { RNLlamaOAICompatibleMessage } from 'llama.rn';\n\nimport { generateChat, isModelReady } from '../llm/inference';\nimport { detectIntentThemes, searchWisdom, type SearchOptions, type ToneTag, type WisdomChunk } from '../retrieval/search';\nimport type { TonePreference, UserPreferences } from '../storage/store';\n\nexport type CitedVerse = { content: string; sourceRef: string };\n\nexport type AssistantResult = {\n  content: string;\n  citedVerses: CitedVerse[];\n};\n\n// Context window configuration\nconst CONTEXT_WINDOW_SIZE = 20; // Number of recent messages to include\nconst SUMMARIZATION_THRESHOLD = 10; // Summarize older messages when history exceeds this\nconst SUMMARY_MESSAGE_COUNT = 10; // Number of older messages to summarize into one\n\n/**\n * Summarizes a batch of older messages into a concise context summary.\n * This helps maintain coherence across longer conversations while staying within token limits.\n */\nfunction summarizeOlderMessages(\n  messages: Array<{ role: 'user' | 'assistant'; content: string }>\n): string {\n  // Extract key topics and themes from older messages\n  const userMessages = messages.filter(m => m.role === 'user').map(m => m.content);\n  const assistantMessages = messages.filter(m => m.role === 'assistant').map(m => m.content);\n\n  // Create a condensed summary of the conversation context\n  const userTopics = userMessages\n    .map(content => {\n      // Extract first sentence or up to 100 chars as topic indicator\n      const firstSentence = content.split(/[.!?]/)[0];\n      return firstSentence.length > 100 ? firstSentence.slice(0, 100) + '...' : firstSentence;\n    })\n    .filter(topic => topic.trim().length > 0);\n\n  // Extract guidance themes from assistant responses\n  const guidanceThemes = assistantMessages\n    .map(content => {\n      // Look for the Guidance section\n      const guidanceMatch = content.match(/Guidance:([^]*?)(?=Reflection:|Practice:|$)/i);\n      if (guidanceMatch) {\n        const guidance = guidanceMatch[1].trim();\n        return guidance.length > 150 ? guidance.slice(0, 150) + '...' : guidance;\n      }\n      return null;\n    })\n    .filter((theme): theme is string => theme !== null);\n\n  // Build summary\n  const summaryParts: string[] = [];\n\n  if (userTopics.length > 0) {\n    summaryParts.push(`Earlier topics discussed: ${userTopics.slice(0, 3).join('; ')}`);\n  }\n\n  if (guidanceThemes.length > 0) {\n    summaryParts.push(`Previous guidance themes: ${guidanceThemes.slice(0, 2).join('; ')}`);\n  }\n\n  return summaryParts.length > 0\n    ? `[Context from earlier in conversation: ${summaryParts.join('. ')}]`\n    : '';\n}\n\n/**\n * Prepares messages for the LLM context with smart summarization.\n * - For conversations <= SUMMARIZATION_THRESHOLD: includes all recent messages\n * - For longer conversations: summarizes older messages and includes recent ones\n */\nfunction prepareContextMessages(\n  previousMessages: Array<{ role: 'user' | 'assistant'; content: string }>\n): Array<{ role: 'user' | 'assistant' | 'system'; content: string }> {\n  // Get the most recent messages up to CONTEXT_WINDOW_SIZE\n  const recentMessages = previousMessages.slice(-CONTEXT_WINDOW_SIZE);\n\n  // If we have more messages than the threshold, create a summary of older messages\n  if (previousMessages.length > SUMMARIZATION_THRESHOLD) {\n    const olderMessages = previousMessages.slice(\n      Math.max(0, previousMessages.length - CONTEXT_WINDOW_SIZE - SUMMARY_MESSAGE_COUNT),\n      previousMessages.length - SUMMARIZATION_THRESHOLD\n    );\n\n    if (olderMessages.length > 0) {\n      const summary = summarizeOlderMessages(olderMessages);\n      if (summary) {\n        // Prepend summary as a system-style context marker within user message\n        // We return it separately to be inserted appropriately\n        return [\n          { role: 'system' as const, content: summary },\n          ...recentMessages.map(m => ({ role: m.role, content: m.content }))\n        ];\n      }\n    }\n  }\n\n  return recentMessages.map(m => ({ role: m.role, content: m.content }));\n}"
}
Now let me update the `generateAssistantResult` function to use the new context preparation:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/service.ts",
  "old_string": "  const system = buildSystemPrompt(preferences, passages.slice(0, 5));\n  const messages: RNLlamaOAICompatibleMessage[] = [\n    { role: 'system', content: system },\n    ...previousMessages.slice(-10).map((m) => ({ role: m.role, content: m.content })),\n    { role: 'user', content: userText },\n  ];\n\n  console.log('[Sage] Generating with context:', messages.length, 'messages');",
  "new_string": "  const system = buildSystemPrompt(preferences, passages.slice(0, 5));\n\n  // Prepare context with smart summarization for longer conversations\n  const contextMessages = prepareContextMessages(previousMessages);\n\n  // Build the full message array\n  const messages: RNLlamaOAICompatibleMessage[] = [\n    { role: 'system', content: system },\n  ];\n\n  // Add context summary if present (it will be the first element with role 'system')\n  const contextSummary = contextMessages.find(m => m.role === 'system');\n  if (contextSummary) {\n    // Append context summary to the system prompt for seamless integration\n    messages[0] = {\n      role: 'system',\n      content: `${system}\\n\\n${contextSummary.content}`,\n    };\n  }\n\n  // Add the conversation history (excluding any system summary)\n  const conversationHistory = contextMessages.filter(m => m.role !== 'system');\n  messages.push(\n    ...conversationHistory.map((m) => ({ role: m.role as 'user' | 'assistant', content: m.content })),\n    { role: 'user', content: userText },\n  );\n\n  console.log('[Sage] Generating with context:', messages.length, 'messages (window:', CONTEXT_WINDOW_SIZE, ', with summarization:', contextSummary ? 'yes' : 'no', ')');"
}
Let me also export the constants so they can be used for testing and configuration purposes. Let me read the file to verify the changes:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/service.ts"
}
The implementation looks good. Now let me export the configuration constants and utility functions for testing purposes:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/service.ts",
  "old_string": "// Context window configuration\nconst CONTEXT_WINDOW_SIZE = 20; // Number of recent messages to include\nconst SUMMARIZATION_THRESHOLD = 10; // Summarize older messages when history exceeds this\nconst SUMMARY_MESSAGE_COUNT = 10; // Number of older messages to summarize into one",
  "new_string": "// Context window configuration (exported for testing)\nexport const CONTEXT_WINDOW_SIZE = 20; // Number of recent messages to include\nexport const SUMMARIZATION_THRESHOLD = 10; // Summarize older messages when history exceeds this\nexport const SUMMARY_MESSAGE_COUNT = 10; // Number of older messages to summarize into one"
}
Now let me also export the helper functions for testing:

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/service.ts",
  "old_string": "/**\n * Summarizes a batch of older messages into a concise context summary.\n * This helps maintain coherence across longer conversations while staying within token limits.\n */\nfunction summarizeOlderMessages(",
  "new_string": "/**\n * Summarizes a batch of older messages into a concise context summary.\n * This helps maintain coherence across longer conversations while staying within token limits.\n */\nexport function summarizeOlderMessages("
}

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/service.ts",
  "old_string": "/**\n * Prepares messages for the LLM context with smart summarization.\n * - For conversations <= SUMMARIZATION_THRESHOLD: includes all recent messages\n * - For longer conversations: summarizes older messages and includes recent ones\n */\nfunction prepareContextMessages(",
  "new_string": "/**\n * Prepares messages for the LLM context with smart summarization.\n * - For conversations <= SUMMARIZATION_THRESHOLD: includes all recent messages\n * - For longer conversations: summarizes older messages and includes recent ones\n */\nexport function prepareContextMessages("
}
Now let me update the exports in the index file to include the new exports:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/index.ts"
}

ðŸ”§ Tool: Edit
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/index.ts",
  "old_string": "export { generateSageReply, type ChatTone, type SageReply, type SageReplyOptions } from './sage';",
  "new_string": "export { generateSageReply, type ChatTone, type SageReply, type SageReplyOptions } from './sage';\nexport {\n  generateAssistantResult,\n  prepareContextMessages,\n  summarizeOlderMessages,\n  CONTEXT_WINDOW_SIZE,\n  SUMMARIZATION_THRESHOLD,\n  SUMMARY_MESSAGE_COUNT,\n  type AssistantResult,\n  type CitedVerse,\n} from './service';"
}

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "content": "Explore codebase to understand existing chat/context structure",
      "status": "completed",
      "activeForm": "Exploring codebase for chat/context structure"
    },
    {
      "content": "Identify where context window is defined and how messages are managed",
      "status": "completed",
      "activeForm": "Identifying context window configuration"
    },
    {
      "content": "Implement expansion from 10 to 20 messages with smart summarization",
      "status": "completed",
      "activeForm": "Implementing context window expansion"
    },
    {
      "content": "Verify implementation with Playwright test",
      "status": "in_progress",
      "activeForm": "Verifying with Playwright test"
    }
  ]
}
Now let me check if Playwright is installed and create a verification test. First, let me check the project structure for existing test setup:

ðŸ”§ Tool: Bash
Input: {
  "command": "cat package.json | grep -A 30 '\"devDependencies\"' || cat package.json",
  "description": "Check package.json for test setup"
}

ðŸ”§ Tool: Glob
Input: {
  "pattern": "**/*test*.{ts,js,tsx,jsx}",
  "path": "/Users/vimo/Projects/OpenCode/Sage_AI"
}
Playwright is installed. Let me check for existing Playwright config and test structure:

ðŸ”§ Tool: Glob
Input: {
  "pattern": "playwright.config.*",
  "path": "/Users/vimo/Projects/OpenCode/Sage_AI"
}

ðŸ”§ Tool: Glob
Input: {
  "pattern": "jest.config.*",
  "path": "/Users/vimo/Projects/OpenCode/Sage_AI"
}
Let me check the jest.config.js to understand the test setup:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/jest.config.js"
}
The project uses Jest with jest-expo preset. Since Playwright is typically used for E2E testing in web applications but this is a React Native app, I'll use Jest instead which is the appropriate testing framework for this codebase. Let me create a Jest test for the context window feature:

ðŸ”§ Tool: Write
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/service.test.ts",
  "content": "/**\n * Verification test for conversation context window expansion feature.\n * This test verifies the expansion from 10 to 20 messages with smart summarization.\n */\n\nimport {\n  CONTEXT_WINDOW_SIZE,\n  SUMMARIZATION_THRESHOLD,\n  SUMMARY_MESSAGE_COUNT,\n  prepareContextMessages,\n  summarizeOlderMessages,\n} from './service';\n\ndescribe('Conversation Context Window Feature', () => {\n  describe('Configuration constants', () => {\n    it('should have CONTEXT_WINDOW_SIZE set to 20', () => {\n      expect(CONTEXT_WINDOW_SIZE).toBe(20);\n    });\n\n    it('should have SUMMARIZATION_THRESHOLD set to 10', () => {\n      expect(SUMMARIZATION_THRESHOLD).toBe(10);\n    });\n\n    it('should have SUMMARY_MESSAGE_COUNT set to 10', () => {\n      expect(SUMMARY_MESSAGE_COUNT).toBe(10);\n    });\n  });\n\n  describe('summarizeOlderMessages', () => {\n    it('should return empty string for empty messages', () => {\n      const result = summarizeOlderMessages([]);\n      expect(result).toBe('');\n    });\n\n    it('should extract topics from user messages', () => {\n      const messages = [\n        { role: 'user' as const, content: 'I am feeling anxious about my work situation.' },\n        { role: 'assistant' as const, content: 'Guidance:\\nLet me help you process this anxiety.' },\n      ];\n\n      const result = summarizeOlderMessages(messages);\n      expect(result).toContain('Earlier topics discussed');\n      expect(result).toContain('I am feeling anxious about my work situation');\n    });\n\n    it('should extract guidance themes from assistant messages', () => {\n      const messages = [\n        { role: 'user' as const, content: 'Help me with stress' },\n        { role: 'assistant' as const, content: 'Guidance:\\nTake a deep breath and center yourself.\\n\\nReflection:\\n- What is causing this stress?' },\n      ];\n\n      const result = summarizeOlderMessages(messages);\n      expect(result).toContain('Previous guidance themes');\n      expect(result).toContain('Take a deep breath');\n    });\n\n    it('should truncate long user topics', () => {\n      const longContent = 'A'.repeat(150);\n      const messages = [\n        { role: 'user' as const, content: longContent },\n      ];\n\n      const result = summarizeOlderMessages(messages);\n      expect(result).toContain('...');\n    });\n\n    it('should limit to 3 user topics', () => {\n      const messages = [\n        { role: 'user' as const, content: 'Topic one about stress.' },\n        { role: 'user' as const, content: 'Topic two about relationships.' },\n        { role: 'user' as const, content: 'Topic three about work.' },\n        { role: 'user' as const, content: 'Topic four about health.' },\n        { role: 'user' as const, content: 'Topic five about money.' },\n      ];\n\n      const result = summarizeOlderMessages(messages);\n      // Should only have first 3 topics\n      expect(result).toContain('Topic one');\n      expect(result).toContain('Topic two');\n      expect(result).toContain('Topic three');\n      expect(result).not.toContain('Topic four');\n      expect(result).not.toContain('Topic five');\n    });\n  });\n\n  describe('prepareContextMessages', () => {\n    it('should return all messages when under threshold', () => {\n      const messages = [\n        { role: 'user' as const, content: 'Message 1' },\n        { role: 'assistant' as const, content: 'Response 1' },\n        { role: 'user' as const, content: 'Message 2' },\n        { role: 'assistant' as const, content: 'Response 2' },\n      ];\n\n      const result = prepareContextMessages(messages);\n      expect(result).toHaveLength(4);\n      expect(result.every(m => m.role !== 'system')).toBe(true);\n    });\n\n    it('should include up to CONTEXT_WINDOW_SIZE (20) recent messages', () => {\n      // Create 25 messages\n      const messages = Array.from({ length: 25 }, (_, i) => ({\n        role: (i % 2 === 0 ? 'user' : 'assistant') as 'user' | 'assistant',\n        content: `Message ${i + 1}`,\n      }));\n\n      const result = prepareContextMessages(messages);\n\n      // Should have at most 20 conversation messages plus potentially 1 system summary\n      const conversationMessages = result.filter(m => m.role !== 'system');\n      expect(conversationMessages.length).toBeLessThanOrEqual(CONTEXT_WINDOW_SIZE);\n    });\n\n    it('should add summarization for conversations exceeding threshold', () => {\n      // Create 15 messages (exceeds threshold of 10)\n      const messages = Array.from({ length: 15 }, (_, i) => ({\n        role: (i % 2 === 0 ? 'user' : 'assistant') as 'user' | 'assistant',\n        content: i % 2 === 0\n          ? `User question ${i + 1} about life.`\n          : `Guidance:\\nHelpful response ${i + 1}.\\n\\nReflection:\\nThink deeply.`,\n      }));\n\n      const result = prepareContextMessages(messages);\n\n      // Should have a system message with summary when > threshold\n      const systemMessages = result.filter(m => m.role === 'system');\n      expect(systemMessages.length).toBe(1);\n      expect(systemMessages[0].content).toContain('Context from earlier in conversation');\n    });\n\n    it('should preserve message order in result', () => {\n      const messages = [\n        { role: 'user' as const, content: 'First user' },\n        { role: 'assistant' as const, content: 'First assistant' },\n        { role: 'user' as const, content: 'Second user' },\n        { role: 'assistant' as const, content: 'Second assistant' },\n      ];\n\n      const result = prepareContextMessages(messages);\n      const conversationMessages = result.filter(m => m.role !== 'system');\n\n      expect(conversationMessages[0].content).toBe('First user');\n      expect(conversationMessages[1].content).toBe('First assistant');\n      expect(conversationMessages[2].content).toBe('Second user');\n      expect(conversationMessages[3].content).toBe('Second assistant');\n    });\n\n    it('should handle exactly SUMMARIZATION_THRESHOLD messages without summary', () => {\n      const messages = Array.from({ length: SUMMARIZATION_THRESHOLD }, (_, i) => ({\n        role: (i % 2 === 0 ? 'user' : 'assistant') as 'user' | 'assistant',\n        content: `Message ${i + 1}`,\n      }));\n\n      const result = prepareContextMessages(messages);\n      const systemMessages = result.filter(m => m.role === 'system');\n\n      // At exactly threshold, no summarization needed\n      expect(systemMessages.length).toBe(0);\n    });\n\n    it('should properly handle 20 message window for long conversations', () => {\n      // Create 30 messages (well over the window)\n      const messages = Array.from({ length: 30 }, (_, i) => ({\n        role: (i % 2 === 0 ? 'user' : 'assistant') as 'user' | 'assistant',\n        content: `Message ${i + 1}`,\n      }));\n\n      const result = prepareContextMessages(messages);\n      const conversationMessages = result.filter(m => m.role !== 'system');\n\n      // Should get exactly 20 most recent messages\n      expect(conversationMessages.length).toBe(20);\n\n      // Last message should be Message 30\n      expect(conversationMessages[conversationMessages.length - 1].content).toBe('Message 30');\n\n      // First conversation message should be Message 11 (30 - 20 + 1)\n      expect(conversationMessages[0].content).toBe('Message 11');\n    });\n  });\n});\n"
}
Now let me run the tests to verify the implementation:

ðŸ”§ Tool: Bash
Input: {
  "command": "npx jest lib/chat/service.test.ts --no-coverage 2>&1",
  "description": "Run Jest tests for service.ts",
  "timeout": 120000
}
All 14 tests pass. Now let me delete the test file as requested:

ðŸ”§ Tool: Bash
Input: {
  "command": "rm /Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/service.test.ts",
  "description": "Delete verification test file"
}

ðŸ”§ Tool: TodoWrite
Input: {
  "todos": [
    {
      "content": "Explore codebase to understand existing chat/context structure",
      "status": "completed",
      "activeForm": "Exploring codebase for chat/context structure"
    },
    {
      "content": "Identify where context window is defined and how messages are managed",
      "status": "completed",
      "activeForm": "Identifying context window configuration"
    },
    {
      "content": "Implement expansion from 10 to 20 messages with smart summarization",
      "status": "completed",
      "activeForm": "Implementing context window expansion"
    },
    {
      "content": "Verify implementation with Playwright test",
      "status": "completed",
      "activeForm": "Verifying with Playwright test"
    }
  ]
}
Let me verify the final state of the modified files:

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/service.ts"
}

ðŸ”§ Tool: Read
Input: {
  "file_path": "/Users/vimo/Projects/OpenCode/Sage_AI/lib/chat/index.ts"
}
<summary>
## Summary: Expand Conversational Context Window from 10 to 20 Messages with Smart Summarization

### Changes Implemented
- Expanded the conversational context window from 10 to 20 messages (`CONTEXT_WINDOW_SIZE = 20`)
- Added smart summarization for older messages when conversation exceeds 10 messages (`SUMMARIZATION_THRESHOLD = 10`)
- Implemented `summarizeOlderMessages()` function that:
  - Extracts key topics from user messages (up to 3 topics, truncated to 100 chars)
  - Extracts guidance themes from assistant responses (up to 2 themes, truncated to 150 chars)
  - Creates a condensed context summary for longer conversations
- Implemented `prepareContextMessages()` function that:
  - Returns up to 20 recent messages for the LLM context
  - Adds a context summary for older messages when threshold is exceeded
  - Maintains message order and conversation coherence
- Updated `generateAssistantResult()` to use the new context preparation system
- Exported new constants and utility functions for testing and configuration

### Files Modified
- `lib/chat/service.ts` - Added context window configuration, summarization logic, and updated message preparation
- `lib/chat/index.ts` - Added exports for new functions and constants

### Verification Status
- Created comprehensive Jest test suite with 14 test cases covering:
  - Configuration constants verification (CONTEXT_WINDOW_SIZE = 20)
  - Summarization logic (topic extraction, truncation, limits)
  - Context message preparation (threshold handling, message ordering, window limits)
- All 14 tests passed successfully
- Test file deleted after verification as requested

### Notes for Developer
- The system now supports up to 20 messages in the LLM context window (doubled from 10)
- When conversations exceed 10 messages, older messages (up to 10) are summarized into a context note
- The summary is appended to the system prompt, maintaining a natural flow
- Console logging updated to show window size and summarization status for debugging
- All configuration constants are exported for easy adjustment and testing
</summary>